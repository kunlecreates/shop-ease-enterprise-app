name: CD â€” Kubernetes Deploy (Helm)

on:
  workflow_dispatch:
    inputs:
      services:
        description: "Comma-separated services to deploy (user,product,order,notification,frontend or 'all')"
        required: true
        default: "all"
      image_tag:
        description: "Container image tag (immutable, e.g. staging-<commit-sha> or full digest). Required for manual deploys."
        required: true
      values_file:
        description: "Path to Helm values file (defaults to helm-charts/values-staging.yaml)"
        required: false
      allow_latest:
        description: "Allow deploying the 'latest' image tag (dangerous)."
        required: false
        default: "false"


jobs:
  prepare:
    runs-on: arc-runnerset-instance
    outputs:
      kubeconfig: ${{ steps.configure.outputs.kubeconfig }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.15.3

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.30.0

      - name: Configure kubeconfig (ARC in-cluster only)
        id: configure
        run: |
          set -euo pipefail
          if [ -z "${KUBERNETES_SERVICE_HOST:-}" ]; then
            echo "ERROR: This job requires running inside the cluster on an ARC runner (KUBERNETES_SERVICE_HOST not set)."
            exit 1
          fi
          echo "Configuring in-cluster kubeconfig (ARC runner)"
          APISERVER="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
          SA_TOKEN_FILE="/var/run/secrets/kubernetes.io/serviceaccount/token"
          CA_CERT_FILE="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          KUBECONFIG_PATH="$PWD/kubeconfig"
          kubectl config set-cluster arc --server="$APISERVER" --certificate-authority="$CA_CERT_FILE" --embed-certs=true --kubeconfig="$KUBECONFIG_PATH"
          kubectl config set-credentials runner --token="$(cat "$SA_TOKEN_FILE")" --kubeconfig="$KUBECONFIG_PATH"
          kubectl config set-context arc --cluster=arc --user=runner --kubeconfig="$KUBECONFIG_PATH"
          kubectl config use-context arc --kubeconfig="$KUBECONFIG_PATH"
          echo "KUBECONFIG=$KUBECONFIG_PATH" >> $GITHUB_ENV
          echo "kubeconfig=$KUBECONFIG_PATH" >> $GITHUB_OUTPUT

      - name: Ensure rbac release namespace exists
        run: kubectl create namespace shopease-system --dry-run=client -o yaml | kubectl apply -f -
        env:
          KUBECONFIG: ${{ steps.configure.outputs.kubeconfig }}

      - name: Create selected service namespaces
        run: |
          export KUBECONFIG=${{ steps.configure.outputs.kubeconfig }}
          INPUT_SERVICES='${{ github.event.inputs.services }}'
          [[ -z "$INPUT_SERVICES" ]] && INPUT_SERVICES=all
          if [[ "$INPUT_SERVICES" == "all" ]]; then
            NAMESPACES=(shopease-user shopease-product shopease-order shopease-notification shopease-frontend)
          else
            IFS=',' read -r -a items <<< "$INPUT_SERVICES"
            NAMESPACES=()
            for s in "${items[@]}"; do
              case "$s" in
                user) NAMESPACES+=(shopease-user) ;;
                product) NAMESPACES+=(shopease-product) ;;
                order) NAMESPACES+=(shopease-order) ;;
                notification) NAMESPACES+=(shopease-notification) ;;
                frontend) NAMESPACES+=(shopease-frontend) ;;
              esac
            done
          fi
          for ns in "${NAMESPACES[@]}"; do
            kubectl create namespace "$ns" --dry-run=client -o yaml | kubectl apply -f -
          done

      - name: Install/Upgrade RBAC Helm chart (limited to selected namespaces)
        run: |
          export KUBECONFIG=${{ steps.configure.outputs.kubeconfig }}
          set -euo pipefail
          # Build temporary values.yaml containing only the selected targets
          tmpvals=$(mktemp)
          echo "targets:" > "$tmpvals"
          for ns in "${NAMESPACES[@]}"; do
            printf '%s\n' "- namespace: ${ns}" "  serviceAccount: arc-runner-sa" "  subjectNamespace: arc-runners" "  roleName: deployer" "  bindingName: deployer-binding" >> "$tmpvals"
          done
          echo "Using temporary RBAC values file:" "$tmpvals"
          helm lint helm-charts/rbac || true
          # templates validated with `helm lint` above
          helm upgrade --install shopease-rbac helm-charts/rbac \
            --namespace shopease-system --create-namespace --atomic --wait --timeout 5m \
            -f "$tmpvals"
          rm -f "$tmpvals"

  
  deploy-helm:
    runs-on: arc-runnerset-instance
    needs: prepare
    env:
      KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}
    strategy:
      matrix:
        service: [user, product, order, notification, frontend]
      # Limit concurrent matrix jobs to reduce runner/builder/registry contention
      max-parallel: 2
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.15.3

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.30.0

      

      - name: Filter services
        run: |
          input="${{ inputs.services }}"
          [[ -z "$input" ]] && input="all"
          svc="${{ matrix.service }}"
          if [[ "$input" != "all" ]]; then
            IFS=',' read -r -a items <<< "$input"
            keep=0
            for i in "${items[@]}"; do
              [[ "$i" == "$svc" ]] && keep=1
            done
            if [[ $keep -eq 0 ]]; then echo "skip" > skip; fi
          fi
      - name: Skip if filtered
        if: ${{ hashFiles('skip') != '' }}
        run: echo "Skipping ${{ matrix.service }}"

      - name: Compute parameters (per service)
        if: ${{ hashFiles('skip') == '' }}
        run: |
          # Map service to release/namespace and enable flags
          SERVICE="${{ matrix.service }}"
          RELEASE_NAME="shopease-${SERVICE}"
          NAMESPACE="shopease-${SERVICE}"
          VALUES_FILE="${{ inputs.values_file }}"
          if [ -z "$VALUES_FILE" ]; then VALUES_FILE="helm-charts/values-staging.yaml"; fi
          IMAGE_TAG_INPUT="${{ inputs.image_tag }}"
          # If no explicit tag supplied, use a short commit SHA for immutable staging tags
          if [ -z "$IMAGE_TAG_INPUT" ]; then
            # GITHUB_SHA is provided by Actions; take first 7 chars for short SHA
            SHORT_SHA="${GITHUB_SHA:0:7}"
            IMAGE_TAG_INPUT="staging-${SHORT_SHA}"
          fi
          echo "RELEASE_NAME=$RELEASE_NAME" >> $GITHUB_ENV
          echo "NAMESPACE=$NAMESPACE" >> $GITHUB_ENV
          echo "VALUES_FILE=$VALUES_FILE" >> $GITHUB_ENV
          echo "IMAGE_TAG_INPUT=$IMAGE_TAG_INPUT" >> $GITHUB_ENV
        shell: bash

      - name: Validate image tag (prevent accidental 'latest' deploys)
        if: ${{ hashFiles('skip') == '' }}
        run: |
          set -euo pipefail
          # Allow override via repo secret or environment if explicitly intended
          if [ "${IMAGE_TAG_INPUT:-}" = "latest" ] && [ "${ALLOW_LATEST:-false}" != "true" ]; then
            echo "ERROR: IMAGE_TAG_INPUT resolved to 'latest'. Deploys must use an immutable tag."
            echo "To allow 'latest' set ALLOW_LATEST=true in the workflow environment (not recommended)."
            exit 1
          fi
        env:
          ALLOW_LATEST: ${{ github.event.inputs.allow_latest }}

      - name: Ensure namespace exists
        if: ${{ hashFiles('skip') == '' }}
        run: kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}

      

      - name: Create/Update DB Secret from GitHub Secrets (user/product/order only)
        if: ${{ hashFiles('skip') == '' }}
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}
          # Product DB secrets
          PRODUCT_DB_HOST: ${{ secrets.PRODUCT_DB_HOST }}
          PRODUCT_DB_PORT: ${{ secrets.PRODUCT_DB_PORT }}
          PRODUCT_DB_NAME: ${{ secrets.PRODUCT_DB_NAME }}
          PRODUCT_DB_USER: ${{ secrets.PRODUCT_DB_USER }}
          PRODUCT_DB_PASSWORD: ${{ secrets.PRODUCT_DB_PASSWORD }}
          # User DB secrets
          USER_DB_HOST: ${{ secrets.USER_DB_HOST }}
          USER_DB_PORT: ${{ secrets.USER_DB_PORT }}
          USER_DB_NAME: ${{ secrets.USER_DB_NAME }}
          USER_DB_USER: ${{ secrets.USER_DB_USER }}
          USER_DB_PASSWORD: ${{ secrets.USER_DB_PASSWORD }}
          # Order DB secrets
          ORDER_DB_HOST: ${{ secrets.ORDER_DB_HOST }}
          ORDER_DB_PORT: ${{ secrets.ORDER_DB_PORT }}
          ORDER_DB_NAME: ${{ secrets.ORDER_DB_NAME }}
          ORDER_DB_USER: ${{ secrets.ORDER_DB_USER }}
          ORDER_DB_PASSWORD: ${{ secrets.ORDER_DB_PASSWORD }}
        run: |
          set -euo pipefail
          SERVICE="${{ matrix.service }}"
          NAMESPACE="$NAMESPACE"
          case "$SERVICE" in
            product)
              SECRET_NAME="shopease-product-db"
              ;;
            user)
              SECRET_NAME="shopease-user-db"
              ;;
            order)
              SECRET_NAME="shopease-order-db"
              ;;
            *)
              echo "No DB secret required for $SERVICE"; exit 0;
              ;;
          esac
          # Build secret manifest only if any matching GitHub secrets are present
          # Expected envs (set as GitHub Secrets at repo/org):
          #   <SERVICE>_DB_HOST, <SERVICE>_DB_PORT, <SERVICE>_DB_NAME, <SERVICE>_DB_USER, <SERVICE>_DB_PASSWORD
          # Example: PRODUCT_DB_HOST, PRODUCT_DB_USER, PRODUCT_DB_PASSWORD, etc.
          upper=$(echo "$SERVICE" | tr '[:lower:]' '[:upper:]')
          keys=(DB_HOST DB_PORT DB_NAME DB_USER DB_PASSWORD)
          has_any=0
          tmpfile=$(mktemp)
          echo "apiVersion: v1" > "$tmpfile"
          echo "kind: Secret" >> "$tmpfile"
          echo "metadata:" >> "$tmpfile"
          echo "  name: $SECRET_NAME" >> "$tmpfile"
          echo "type: Opaque" >> "$tmpfile"
          echo "stringData:" >> "$tmpfile"
          for k in "${keys[@]}"; do
            varname="${upper}_${k}"
            val="${!varname:-}"
            if [ -n "$val" ]; then
              has_any=1
              # Write keys using the <SERVICE>_DB_* naming convention (e.g. PRODUCT_DB_HOST)
              echo "  ${varname}: \"$val\"" >> "$tmpfile"
            fi
          done
          if [ $has_any -eq 1 ]; then
            kubectl -n "$NAMESPACE" apply -f "$tmpfile"
            rm -f "$tmpfile"
            echo "DB secret $SECRET_NAME applied in $NAMESPACE"
            echo "DB_SECRET_NAME=$SECRET_NAME" >> $GITHUB_ENV
          else
            echo "No GitHub Secrets found for $SERVICE DB; skipping secret creation."
            echo "DB_SECRET_NAME=" >> $GITHUB_ENV
          fi
        # env moved above

      - name: Create/Update Notification credentials Secret from GitHub Secrets (notification only)
        if: ${{ hashFiles('skip') == '' && matrix.service == 'notification' }}
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}
          NOTIFICATION_MAIL_HOST: ${{ secrets.NOTIFICATION_MAIL_HOST }}
          NOTIFICATION_MAIL_PORT: ${{ secrets.NOTIFICATION_MAIL_PORT }}
          NOTIFICATION_MAIL_USER: ${{ secrets.NOTIFICATION_MAIL_USER }}
          NOTIFICATION_MAIL_PASSWORD: ${{ secrets.NOTIFICATION_MAIL_PASSWORD }}
          NOTIFICATION_MAIL_FROM: ${{ secrets.NOTIFICATION_MAIL_FROM }}
        run: |
          set -euo pipefail
          NAMESPACE="$NAMESPACE"
          SECRET_NAME="shopease-notification-credentials"
          # Expected GitHub Secrets (mapped to shorter stringData keys):
          #   MAIL_HOST, MAIL_PORT, MAIL_USER, MAIL_PASSWORD, MAIL_FROM
          tmpfile=$(mktemp)
          echo "apiVersion: v1" > "$tmpfile"
          echo "kind: Secret" >> "$tmpfile"
          echo "metadata:" >> "$tmpfile"
          echo "  name: $SECRET_NAME" >> "$tmpfile"
          echo "type: Opaque" >> "$tmpfile"
          echo "stringData:" >> "$tmpfile"
          has_any=0
          # Use loop like DB secret creation to map NOTIFICATION_<KEY> envs to shorter keys in stringData
          keys=(MAIL_HOST MAIL_PORT MAIL_USER MAIL_PASSWORD MAIL_FROM)
          for k in "${keys[@]}"; do
            varname="NOTIFICATION_${k}"
            val="${!varname:-}"
            if [ -n "$val" ]; then
              has_any=1
              echo "  $k: \"$val\"" >> "$tmpfile"
            fi
          done
          if [ $has_any -eq 1 ]; then
            kubectl -n "$NAMESPACE" apply -f "$tmpfile"
            rm -f "$tmpfile"
            echo "Credentials secret $SECRET_NAME applied in $NAMESPACE"
            echo "CREDENTIALS_SECRET_NAME=$SECRET_NAME" >> $GITHUB_ENV
          else
            echo "No GitHub Secrets found for notification credentials; skipping creation."
            echo "CREDENTIALS_SECRET_NAME=" >> $GITHUB_ENV
          fi

      # Namespace creation moved earlier (immediately after Compute parameters)

      - name: Create/Update imagePullSecret for GHCR (optional)
        if: ${{ hashFiles('skip') == '' }}
        env:
          GHCR_READ_USERNAME: ${{ secrets.GHCR_READ_USERNAME }}
          GHCR_READ_TOKEN: ${{ secrets.GHCR_READ_TOKEN }}
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}
        run: |
          if [ -n "$GHCR_READ_USERNAME" ] && [ -n "$GHCR_READ_TOKEN" ]; then
            kubectl -n "$NAMESPACE" create secret docker-registry ghcr-pull-secret \
              --docker-server=ghcr.io \
              --docker-username="$GHCR_READ_USERNAME" \
              --docker-password="$GHCR_READ_TOKEN" \
              --docker-email="noreply@github.com" \
              --dry-run=client -o yaml | kubectl apply -f -
          else
            echo "GHCR_READ_USERNAME/GHCR_READ_TOKEN not provided; skipping imagePullSecret creation"
          fi
      
      - name: Pre-deploy readiness checks
        if: ${{ hashFiles('skip') == '' }}
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}
        run: |
          set -euo pipefail
          echo "Running pre-deploy readiness checks in namespace $NAMESPACE"

          # Wait for DB secret if provided
          if [ -n "${DB_SECRET_NAME:-}" ]; then
            echo "Checking DB secret: $DB_SECRET_NAME"
            for i in $(seq 1 12); do
              if kubectl -n "$NAMESPACE" get secret "$DB_SECRET_NAME" >/dev/null 2>&1; then
                echo "Found secret $DB_SECRET_NAME"; break
              fi
              echo "Waiting for secret $DB_SECRET_NAME... ($i/12)"; sleep 5
            done
            if ! kubectl -n "$NAMESPACE" get secret "$DB_SECRET_NAME" >/dev/null 2>&1; then
              echo "ERROR: required DB secret $DB_SECRET_NAME not found in $NAMESPACE"; exit 1
            fi
          else
            echo "No DB secret required for this service"
          fi

          # Wait for notification credentials secret if provided
          if [ -n "${CREDENTIALS_SECRET_NAME:-}" ]; then
            echo "Checking credentials secret: $CREDENTIALS_SECRET_NAME"
            for i in $(seq 1 12); do
              if kubectl -n "$NAMESPACE" get secret "$CREDENTIALS_SECRET_NAME" >/dev/null 2>&1; then
                echo "Found secret $CREDENTIALS_SECRET_NAME"; break
              fi
              echo "Waiting for secret $CREDENTIALS_SECRET_NAME... ($i/12)"; sleep 5
            done
            if ! kubectl -n "$NAMESPACE" get secret "$CREDENTIALS_SECRET_NAME" >/dev/null 2>&1; then
              echo "ERROR: required credentials secret $CREDENTIALS_SECRET_NAME not found in $NAMESPACE"; exit 1
            fi
          else
            echo "No credentials secret required for this service"
          fi

          # Check imagePullSecret if exists (optional)
          if kubectl -n "$NAMESPACE" get secret ghcr-pull-secret >/dev/null 2>&1; then
            echo "imagePullSecret ghcr-pull-secret present in $NAMESPACE"
          else
            echo "imagePullSecret ghcr-pull-secret not present; helm may still succeed if images are public"
          fi

          echo "Pre-deploy readiness checks passed."

      - name: Deploy selected service via helm (per-service chart)
        if: ${{ hashFiles('skip') == '' }}
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}
          GHCR_READ_USERNAME: ${{ secrets.GHCR_READ_USERNAME }}
          GHCR_READ_TOKEN: ${{ secrets.GHCR_READ_TOKEN }}
        run: |
          set -euo pipefail
          OWNER="${{ github.repository_owner }}"
          # Optional: read env values from umbrella values-staging.yaml using yq
          # Per-service environment values are now owned by each service's Helm chart
          # (services/<svc>/helm/values.yaml). We don't need to extract `env` from the
          # umbrella values file anymore. If a custom values file was supplied via
          # the `values_file` input, it will be passed directly to Helm below.

          case "${{ matrix.service }}" in
            user)
              CHART_DIR="services/user-service/helm";
              IMAGE_REPO="ghcr.io/${OWNER}/user-service";
              HELM_SET_DB_VAL="db.secretName=${DB_SECRET_NAME}"
              ;;
            product)
              CHART_DIR="services/product-service/helm";
              IMAGE_REPO="ghcr.io/${OWNER}/product-service";
              HELM_SET_DB_VAL="db.secretName=${DB_SECRET_NAME}"
              ;;
            order)
              CHART_DIR="services/order-service/helm";
              IMAGE_REPO="ghcr.io/${OWNER}/order-service";
              HELM_SET_DB_VAL="db.secretName=${DB_SECRET_NAME}"
              ;;
            notification)
              CHART_DIR="services/notification-service/helm";
              IMAGE_REPO="ghcr.io/${OWNER}/notification-service";
              HELM_SET_DB_VAL="credentials.secretName=${CREDENTIALS_SECRET_NAME}"
              ;;
            frontend)
              CHART_DIR="frontend/helm";
              IMAGE_REPO="ghcr.io/${OWNER}/shopease-frontend";
              HELM_SET_DB=""
              ;;
          esac
          # Optional: imagePullSecrets from umbrella values (use ghcr-pull-secret if present)
          HELM_SET_PULLSECRET_VAL="imagePullSecrets[0].name=ghcr-pull-secret"
          helm lint "$CHART_DIR"
          # templates validated with `helm lint` above

          # Build helm args safely into an array to avoid word-splitting and malformed invocations
          HELM_ARGS=( --namespace "$NAMESPACE" --set "image.repository=$IMAGE_REPO" --set "image.tag=$IMAGE_TAG_INPUT" )
          # If an umbrella or custom values file was provided, pass it directly to Helm.
          if [ -n "${VALUES_FILE:-}" ] && [ -f "$VALUES_FILE" ]; then
            HELM_ARGS+=( -f "$VALUES_FILE" )
          fi
          if [ -n "${HELM_SET_DB_VAL:-}" ]; then
            HELM_ARGS+=( --set "$HELM_SET_DB_VAL" )
          fi
          if [ -n "${HELM_SET_PULLSECRET_VAL:-}" ]; then
            HELM_ARGS+=( --set "$HELM_SET_PULLSECRET_VAL" )
          fi

          # Debug: show resolved values for troubleshooting
          echo "Helm release: $RELEASE_NAME"
          echo "Chart dir: $CHART_DIR"
          echo "Helm args: ${HELM_ARGS[*]}"

          # --- Pre-deploy validations ---
          # 1) Registry manifest exists (authenticated to GHCR when possible)
          repo_name="${IMAGE_REPO##*/}"
          manifest_url="https://ghcr.io/v2/${OWNER}/${repo_name}/manifests/${IMAGE_TAG_INPUT}"
          if [ -n "${GHCR_READ_USERNAME:-}" ] && [ -n "${GHCR_READ_TOKEN:-}" ]; then
            echo "Authenticating to GHCR with provided credentials"
            # Use docker client to perform the token exchange and manifest inspection
            echo "${GHCR_READ_TOKEN}" | docker login ghcr.io -u "${GHCR_READ_USERNAME}" --password-stdin >/dev/null 2>&1 || {
              echo "ERROR: docker login to ghcr.io failed with provided credentials"; exit 1; }

            echo "Inspecting manifest using docker manifest inspect ${IMAGE_REPO}:${IMAGE_TAG_INPUT}"
            if ! docker manifest inspect "${IMAGE_REPO}:${IMAGE_TAG_INPUT}" >/dev/null 2>&1; then
              echo "ERROR: docker manifest inspect failed for ${IMAGE_REPO}:${IMAGE_TAG_INPUT} (missing tag or access denied)."
              docker logout ghcr.io >/dev/null 2>&1 || true
              exit 1
            fi

            # cleanup docker login
            docker logout ghcr.io >/dev/null 2>&1 || true
          else
            echo "GHCR credentials not provided; attempting unauthenticated manifest check for $manifest_url"
            if ! curl -sfI -H "Accept: application/vnd.oci.image.manifest.v1+json" "$manifest_url"; then
              echo "Warning: unauthenticated manifest check failed; image may be private or tag missing. Helm may fail with ImagePull errors."
            fi
          fi

          # 2) If an existing release is present, fail fast when pods show ImagePullBackOff/ErrImagePull
          if helm -n "$NAMESPACE" status "$RELEASE_NAME" >/dev/null 2>&1; then
            echo "Existing release $RELEASE_NAME detected; checking pods for ImagePullBackOff/ErrImagePull"
            bad=$(kubectl -n "$NAMESPACE" get pods -l app.kubernetes.io/instance="$RELEASE_NAME" -o jsonpath='{range .items[*]}{.metadata.name}{":"}{range .status.containerStatuses[*]}{.state.waiting.reason}{";"}{end}{"\n"}{end}' 2>/dev/null || true)
            if printf '%s' "$bad" | egrep -q 'ImagePullBackOff|ErrImagePull'; then
              echo "ERROR: existing release $RELEASE_NAME has pods in ImagePullBackOff/ErrImagePull. Describe pods:"
              kubectl -n "$NAMESPACE" describe pods -l app.kubernetes.io/instance="$RELEASE_NAME" --show-events || true
              helm -n "$NAMESPACE" status "$RELEASE_NAME" || true
              helm -n "$NAMESPACE" history "$RELEASE_NAME" || true
              exit 1
            fi
          fi

          # 3) Run helm upgrade/install and surface helpful diagnostics on concurrency lock
          tmphelmout=$(mktemp)
          if helm upgrade --install "$RELEASE_NAME" "$CHART_DIR" "${HELM_ARGS[@]}" --atomic --wait --timeout 10m 2>&1 | tee "$tmphelmout"; then
            echo "Helm upgrade/install succeeded"
          else
            helm_out=$(cat "$tmphelmout" || true)
            if printf '%s' "$helm_out" | grep -q "another operation (install/upgrade/rollback) is in progress"; then
              echo "ERROR: Helm reported a concurrent operation lock for release $RELEASE_NAME"
              echo "--- Helm status ---"
              helm -n "$NAMESPACE" status "$RELEASE_NAME" || true
              echo "--- Helm history ---"
              helm -n "$NAMESPACE" history "$RELEASE_NAME" || true
              echo "--- Helm output ---"
              printf '%s' "$helm_out"
              rm -f "$tmphelmout" || true
              exit 1
            else
              echo "ERROR: Helm upgrade/install failed with unexpected error:"; printf '%s' "$helm_out"
              rm -f "$tmphelmout" || true
              exit 1
            fi
          fi
          rm -f "$tmphelmout" || true

          # No temporary env values files are created here (per-service charts own env values).

      - name: Wait for rollout
        if: ${{ hashFiles('skip') == '' }}
        run: |
          kubectl -n "$NAMESPACE" rollout status deployment/$(kubectl -n "$NAMESPACE" get deploy -l app.kubernetes.io/instance=$RELEASE_NAME -o jsonpath='{.items[0].metadata.name}') --timeout=600s
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}

      - name: Run Playwright smoke against staging (frontend only)
        if: ${{ matrix.service == 'frontend' && hashFiles('skip') == '' }}
        env:
          BASE_URL: https://staging.shopease.local
        run: |
          cd frontend
          npm ci
          npm i -D @playwright/test
          npx playwright install --with-deps
          npx playwright test --project=chromium

      - name: Smoke test (per service)
        if: ${{ hashFiles('skip') == '' }}
        run: |
          case "${{ matrix.service }}" in
            frontend)
              SERVICE_NAME="frontend"; PORT=80; PATH="/" ;;
            product)
              SERVICE_NAME="product-service"; PORT=80; PATH="/api/product" ;;
            user)
              SERVICE_NAME="user-service"; PORT=80; PATH="/api/user" ;;
            order)
              SERVICE_NAME="order-service"; PORT=80; PATH="/api/order" ;;
            notification)
              SERVICE_NAME="notification-service"; PORT=80; PATH="/api/notification/health" ;;
          esac
          echo "Detected Service: $SERVICE_NAME"
          kubectl -n "$NAMESPACE" run curl-test --restart=Never --image=curlimages/curl:8.8.0 -- sleep 60
          kubectl -n "$NAMESPACE" wait --for=condition=Ready pod/curl-test --timeout=30s
          kubectl -n "$NAMESPACE" exec curl-test -- curl -fsS http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local:$PORT$PATH || (echo "Service unreachable" && exit 1)
          kubectl -n "$NAMESPACE" delete pod curl-test
        env:
          KUBECONFIG: ${{ needs.prepare.outputs.kubeconfig }}